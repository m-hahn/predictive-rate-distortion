%  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

% You need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================
\documentclass[entropy,article,submit,moreauthors,pdftex,10pt,a4paper]{Definitions/mdpi} 

% If you would like to post an early version of this manuscript as a preprint, you may use preprint as the journal and change 'submit' to 'accept'. The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex,10pt,a4paper]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.


%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.



\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2018}
\copyrightyear{2018}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}

\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\Ff}[0]{\mathcal{F}}

\newif \ifcomment
\commenttrue

\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{\ifcomment{{\color{red}#1}}\else{}\fi}
\newcommand\mhahn[1]{\ifcomment{{\color{red}(#1)}}\else{}\fi}
\newcommand\rljf[1]{\ifcomment{{\color{blue}(#1)}}\else{}\fi}

\usepackage{amsthm}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}


%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).


%\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}

\usepackage{tikz}
\usetikzlibrary{automata,positioning}



\usepackage{mathtools}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\newcommand{\future}{\overrightarrow{X}}
\newcommand{\past}{\overleftarrow{X}}
\newcommand{\key}{\textbf}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\finitefuture}{\stackrel{\rightarrow \scriptscriptstyle{M}}{X}}
\newcommand{\finitepast}{\stackrel{\scriptscriptstyle{M}\leftarrow}{X}}%_{-M\dots -1}}


\let\oldequation\equation
\let\oldendequation\endequation

\renewenvironment{equation}
  {\linenomathNonumbers\oldequation}
  {\oldendequation\endlinenomath}

\Title{Estimating Predictive Rate-Distortion Curves via Neural Variational Inference}

\author{Michael Hahn, Richard Futrell}


% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Michael Hahn $^{1}$ and Richard Futrell $^{2}$}

% Authors, for metadata in PDF
\AuthorNames{Michael Hahn and Richard Futrell}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Stanford University; mhahn2@stanford.edu\\
$^{2}$ \quad University of California, Irvine; rfutrell@uci.edu}

% Contact information of the corresponding author
\corres{Correspondence: mhahn2@stanford.edu}

% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{The predictive rate-distortion curve quantifies the tradeoff between compressing information about the past of a stochastic process and predicting its future accurately.
Existing estimation methods for this curve work by clustering finite sequences of observations or by utilizing analytically known causal states.
Neither type of approach scales to processes such as natural languages, which have large alphabets and long dependencies, and where the causal states are not known analytically.
We describe Neural Predictive Rate-Distortion (NPRD), an estimation method that scales to such processes, leveraging the universal approximation capabilities of neural networks.
Taking only time series data as input, the method computes a variational bound on the predictive rate-distortion curve.
We validate the method on processes where predictive rate-distortion is analytically known.
As an application, we provide bounds on the predictive rate-distortion of natural language, improving on bounds provided by clustering sequences.
Based on the results, we argue that the predictive rate-distortion curve is more useful than the usual notion of statistical complexity for characterizing highly complex processes such as natural language.}

% Keywords
\keyword{Predictive rate-distortion; Natural language; Information bottleneck; Neural variational inference} % (list three to ten pertinent keywords specific to the article, yet reasonably common within the subject discipline.)

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}



\begin{document}


% NOTES
% - cite this paper https://arxiv.org/pdf/1705.02436.pdf (they have a different IB bound, but also do neural networks)


Predicting the future from past observations is a key problem across science.
Constructing models that predict future observations is a fundamental method of making and testing scientific discoveries.
Understanding and predicting dynamics has been a fundamental goal of physics for centuries.
In engineering, devices often have to predict the future of their environment to perform efficiently.
Living organisms need to make inferences about their environment and its future for survival.


%Often the underlying process is very complicated.
Real-world systems often generate complex dynamics that can be hard to compute.
A biological organism typically will not have the computational capacity to perfectly represent the environment.
In science, measurements have limited precision, limiting the precision with which one can hope to make predictions.
%Prediction need not be perfect.
In these settings, the main goal will typically be to get good prediction at low computational cost.
%Similarly natural language.



This motivates the study of models that try to extract those key features of past observations that are most relevant to predicting the future.
%This results in a tradeoff between the cost of representing past observations and predicting the future well.
%This tradeoff has been formalized and computationally studied in a range of prior studies.
A general information-theoretic framework for this problem is provided by Predictive Rate-Distortion~~\citep{still-information-2014,marzen-predictive-2016}, also known as the Past-Future Information Bottleneck~\citep{creutzig-past-future-2009}.
The Predictive Rate-Distortion tradeoff seeks to find an encoding of past observations that is maximally informative about future observations, while satisfying a bound on the amount of information that has to be stored.
More formally, this framework trades off prediction loss in the future, formalized as cross-entropy, with the cost of representing the past, formalized as the mutual information between the past observations and the compressed representations of the past.
Due to its information-theoretic nature, this framework is extremely general and applies to processes across vastly different domains.
It has been applied to linear dynamical systems \citep{creutzig-past-future-2009,amir2015past}, but is equally meaningful for discrete dynamical systems~\citep{marzen-predictive-2016}.
For biological systems that make predictions about their environment, this corresponds to placing an information-theoretic constraint on the computational cost used for conditioning actions on past observations~\citep{genewein2015bounded}.

The problem of determining encodings that optimize the Predictive Rate-Distortion tradeoff has been solved for certain specific kinds of dynamics, namely for linear dynamic systems~\citep{creutzig-past-future-2009} and for processes whose predictive dynamic is represented exactly by a known, finite Hidden Markov Model~\citep{marzen-predictive-2016}.
However, real-world processes are often more complicated.
When the dynamics are known, a representing Hidden Markov Model may still be extremely large or even infinite, making general-purpose automated computation difficult.
Even more importantly, the underlying dynamics are often not known exactly.
An organism typically does not have access to the exact dynamics of its surroundings.
Similarly, the exact distribution of sentences in a natural language is not known exactly, precluding the application of methods that require an exact model.
Such processes are typically only available implicitly, through a finite sample of trajectories.



Optimal Causal Filtering (OCF, \citet{still-optimal-2010}) addresses the problem of estimating Predictive Rate-Distortion from a finite sample of observation trajectories.
It does so by constructing a matrix of observed frequencies of different pairs of past and future observations.
However, such a method faces a series of challenges~\citep{marzen-predictive-2016}.
One is the curse of dimensionality:
Modeling long dependencies requires storing an exponential number of observations, which quickly becomes intractable for current computation methods.
This exponential growth is particularly problematic when dealing with processes with a large state space.
For instance, the number of distinct words in a human language as found in large-scale text data easily exceeds $10^5$, making storing and manipulating counts of longer word sequences very challenging.
A second challenge is that of overfitting:
When deploying a predictive model constructed via OCF on new data to predict upcoming observations, such a model can only succeed when the past sequences occurred in the sample that OCF was applied to.
This is because OCF relies on counts of full past and future observation sequences; it does not generalize to unseen past sequences.

Extrapolating to unseen past sequences is possible in traditional time series models representing processes that take continuous values; however, such methods are less easily applied to discrete sequences such as natural language.
Recent research has seen a flurry of interest in using flexible nonlinear function approximators, and in particular recurrent neural networks, which can handle sequences with discrete observations.
Such machine learning methods provide generic models of sequence data.
They are the basis of the state of the art by a clear and significant margin for prediction in natural language \citep{DBLP:journals/corr/JozefowiczVSSW16, merity2018analysis, dai2019transformer,takahashi2018cross}.
They also have been successfully applied to modeling many other kinds of time series found across disciplines~\citep{ogunmolu2016nonlinear,laptev2017time,meyer2018survey,hundman2018detecting,white2018forecasting,woo2018dynamic,sirignano2018universal,mohajerin2019multistep}. %TODO more details
% ,sirignano2018universal,hagge2017solving
%, such as financial time series~\cite{} and purely theoretical chaotic dynamical systems

We propose \key{Neural Predictive Rate-Distortion} (NPRD) to estimate predictive rate-distortion when only a finite set of sample trajectories is given.
We use neural networks both to encode past observations into a summary code, and to predict future observations from it.
The universal function approximation capabilities of neural networks enable such networks to capture complex dynamics, with computational cost scaling only linearly with the length of observed trajectories, compared to the exponential cost of OCF.
When deploying on new data, such a neural model can generalize seamlessly to unseen sequences, and generate appropriate novel summary encodings on the fly.
Recent advances in neural variational inference~\citep{rezende-variational-2015,huang-neural-2018} allow us to construct predictive models that provide almost optimal predictive performance at a given rate, and to estimate the predictive rate-distortion tradeoff from such networks.
Our method can be applied to sample trajectories in an off-the-shelf manner, without prior knowledge of the underlying process.

In Section~\ref{sec:background}, we formally introduce predictive rate-distortion, and discuss related notions of predictive complexity.
In Section~\ref{sec:OCF}, we describe the prior method of Optimal Causal Filtering (OCF).
In Section~\ref{sec:nprd}, we describe our method NPRD.
In Section~\ref{sec:experiments}, we validate NPRD on processes whose predictive rate-distortion is analytically known, showing that it finds essentially optimal predictive models.
In Section~\ref{sec:language}, we apply NPRD to data from five natural languages, providing the first estimates of predictive rate-distortion of natural language.


%\paragraph{Marginal $q$}
%For $q$, we choose the family of \emph{Neural Autoregressive Flows} \citep{}. 
%This is a parametric family of distributions that allows efficient estimation of the probability density and its gradients.
%This method widely generalizes a family of prior methods \citep{, kingma-improving-2016, papamakarios-masked-2017}, offering efficient estimation while surpassing prior methods in expressivity.
%TODO here is some more stuff:
%There are several motivations for studying this predictive rate-distortion tradeoff.
%One is that the shape of the tradeoff can provide insights into the structure of the process (TODO citations).
%%(CITE) show that phase transitions in the tradeoff curve reveal ...
%Second, this problem formalizes tradeoffs observed in real-world situations, where the goal is to predict the next observations in a sequence.
%Statistically modeling natural language, constructing models of the probability distribution over different sentences in a language, is often implemented as the problem of sequentially predicting the next word from the prior context.
%There is a tradeoff between the computational cost of representing the prior context, and accurately modeling the conditional distribution over the next word.
%Third, studying tis tradeoff may provide simpler but faithful models of processes (TODO citations).

% Some other stuff to cite/consider in framing
% - the Tim Genewein papers on bounded rationality -- predictive rate distortion is like the action of an boundedly rational predictor 
% - Rubin, Shamir & Tishby (2010). Trading value and information in MDPs. Is predictive rate distortion the same as their model but with KL divergence as the value function?

%In the modeling of stochastic processes, Predictive Rate-Distortion describes the tradeoff between the cost of encoding the past, and the benefit of predicting the future better.
%It is formalized as the tradeoff between the mutual information of an encoding with the past and with the future of the process, an application of the Information Bottleneck to processes \citep{tishby-information-1999}.



\section{Predictive Rate-Distortion}\label{sec:background}

We consider stationary discrete-time stochastic processes $(X_t)_{t \in \mathbb{Z}}$, taking values in a state space $S$.
Given a reference point in time, say $T=0$, we are interested in the problem of predicting the future of $X_{t\geq 0} = (X_0, X_1, X_2, ...)$ from the past $X_{t< 0} = (..., X_{-2}, X_{-1})$.
In general---unless the observations $X_t$ are independent--- predicting the future of the process accurately will require taking into account the past observations.
There is a tradeoff between the accuracy of prediction, and how much information about the past is being taken into account.
On one extreme, not taking the past into account at all, one will not be able to take advantage of the dependencies between past and future observations.
On the other extreme, considering the entirety of the past observations $X_{t \leq 0}$ can require storing large and potentially unbounded amounts of information.
This tradeoff between information storage and prediction accuracy is referred to as \key{Predictive Rate-Distortion} (PRD) \citep{marzen-predictive-2016}.
The term \key{rate} refers to the amount of past information being taken into account, while \key{distortion} refers to the degradation in prediction compared to optimal prediction from the full past.



The problem of predictive rate-distortion has been formalized by a range of studies.
A principled and general formalization is provided by applying the Information Bottleneck idea \citep{tishby-information-1999, still-optimal-2010,marzen-predictive-2016}:
We will write $\past$ for the past $X_{< 0}$, and $\future$ for the future $X_{\geq 0}$, following \cite{marzen-predictive-2016}.
We consider random variables $Z$, called \key{codes}, that summarize the past and are used by the observer to predict the future.
Formally, $Z$ needs to be independent of the future $\future$ conditional on the past $\past$: in other words, $Z$ does not provide any information about the future except what is contained in the past. Symbolically:
\begin{equation}\label{eq:markov}
	Z \bot \future | \past.
\end{equation}
This is equivalent to the requirement that $Z \leftrightarrow \past \leftrightarrow \future$ be a Markov chain.
This formalizes the idea that the code is computed by an observer from the past, without having access to the future.
Predictions are then made based only on $Z$, without additional access to the past $\past$.

The rate of the code $Z$ is the mutual information between $Z$ and the past: $\operatorname{I}[Z, \past]$.
By the Channel Coding Theorem, this describes the channel capacity that the observer requires in order to transform past observations into the code $Z$.

The distortion is the loss in predictive accuracy when predicting from $Z$, relative to optimal prediction from the full past $\past$.
In the Information Bottleneck formalization, this is equivalent to the amount of mutual information between past and future that is \emph{not} captured by $Z$ \citep{harremoes2007information}: 
\begin{equation}
	\operatorname{I}[\past, \future|Z].
\end{equation}
Due to the Markov condition, the distortion measure satisfies the relation
\begin{equation}
	\operatorname{I}[\past, \future|Z] = \operatorname{I}[\past, \future] - \operatorname{I}[Z, \future],
\end{equation}
i.e., it captures how much less information $Z$ carries about the future $\future$ compared to the full past $\past$.
For a fixed process $(X_t)_t$, choosing $Z$ to minimize the distortion is equivalent to maximizing the mutual information between the code and the future:
\begin{equation}\label{eq:predictiveness}
\operatorname{I}[Z, \future].
\end{equation}
We will refer to~(\ref{eq:predictiveness}) as the \key{predictiveness} of the code $Z$.

The rate-distortion tradeoff then chooses $Z$ to %minimize rate at bounded distortion:
%$$\min_{Z : \operatorname{I}[\past, \future|Z] \leq d} \operatorname{I}[Z, \past]$$
%, or -- equivalently --
 minimize distortion at bounded rate:
\begin{equation}
\min_{Z :  \operatorname{I}[\past, Z] \leq d}  \operatorname{I}[\past, \future|Z] 
\end{equation}
or -- equivalently -- maximize predictiveness at bounded rate:
\begin{equation}
\max_{Z :  \operatorname{I}[\past, Z] \leq d}  \operatorname{I}[Z, \future].
\end{equation}
Equivalently, for each  $\lambda \geq 0$, we study the problem 
\begin{equation}\label{eq:ib}
	\max_{Z} \left( \operatorname{I}[Z, \future] - \lambda \cdot \operatorname{I}[ \past, Z] \right),
\end{equation}
where the scope of the maximization is the class of all random variables $Z$ such that $Z \rightarrow \past \rightarrow \future$ is a Markov chain. %, that is, $Z$ is independent of the future given the past.

The objective (\ref{eq:ib}) is equivalent to the Information Bottleneck \citep{tishby-information-1999}, applied to the past and future of a stochastic process.
The coefficient $\lambda$ indicates how strongly a high rate $\operatorname{I}[\past, Z]$ is penalized; higher values of $\lambda$ result in lower rates and thus lower values of predictiveness.

The largest achievable predictiveness $\operatorname{I}[Z, \future]$ is equal to $\operatorname{I}[\past, \future]$, which is known as the \key{excess entropy} of the process~\citep{feldman-synchronizing-2004}.
Due to the Markov condition~(\ref{eq:markov}) and the Data Processing Inequality, predictiveness of a code $Z$ is always upper-bounded by the rate:
\begin{equation}
\operatorname{I}[Z, \future] \leq \operatorname{I}[\past, Z] 
\end{equation}
As a consequence, when $\lambda \geq 1$, then (\ref{eq:ib}) is always optimized by a trivial $Z$ with zero rate and zero predictiveness.
When $\lambda = 0$, any lossless code optimizes the problem.
Therefore, we will be concerned with the situation where $\lambda \in (0,1)$.

%
%
%Predictive rate-distortion describes the tradeoff between the cost of representing features of the past, and the quality of predictions in the future.
%Let $(X_t)$ be a stationary process.
%We consider feature representations $Z$ of the past $\past$ that are used to predict the future $\future$.
%There is a tradeoff between the complexity of $Z$ and the success of predicting the future:
%More complex representations of the past can reduce future prediction loss -- up to a certain limit given by the entropy rate of the process. %This limit corresponds to perfect modeling, and results in Crutchfield and colleagues' notion of complexity.
%
%The complexity of the representation $Z$, or the \emph{rate}, is formalized as the mutual information between the past and the representation $Z$: $\operatorname{I}[Z, \past]$.
%Different distortion measures could be used as measures of prediction loss.
%CITE maximize the entropy, or, equivalently, minimize the mutual information, between $Z$ and the future: $\operatorname{I}[Z, \past]$.
%
%Thus, for any process $(X_t)$, we obtain a rate-distortion tradeoff curve.
%The achievable region is populated by the random variables $Z$ such that $Z \rightarrow \left(\past\right) \rightarrow \left(\future\right)$ is a Markov chain.
%The points on the tradeoff curve are obtained by solving, for each $\beta \in [0,1]$, the problem
%\begin{equation}\label{eq:ib}
%	\max_{Z} \left( \operatorname{I}[\future, Z] - \beta \cdot \operatorname{I}[Z, (\past)] \right)
%\end{equation}
%where the scope of the maximization is the class of all random variables $Z$ such that $Z - \left(\past\right) - \left(\future\right)$ is a Markov chain, that is, $Z$ is independent of the future given the past.
%This is exactly the Information Bottleneck \cite{tishby-information-1999}, applied to the past and future of a stochastic process.
%
%

\subsection{Relation to Statistical Complexity}\label{sec:complexity}

Predictive Rate-Distortion is closely related to Statistical Complexity and the $\epsilon$-machine \citep{crutchfield-inferring-1989,Grassberger1986}. 
Given a stationary process $X_t$, its \key{causal states} are the equivalence classes of semi-infinite pasts $\past$ that induce the same conditional probability over semi-infinite futures $\future$:
Two pasts $\past, \past^\prime$ belong to the same causal state if and only if $P(x_{1...k}|\past) = P(x_{1...k}|\past^\prime)$ holds for all finite sequences $x_{0...k}$ ($k \in \mathbb{N}$).
Note that this definition is not measure-theoretically rigorous; such a treatment is provided by~\citet{lohr-properties-2009}.

The causal states constitute the state set of a a Hidden Markov Model (HMM) for the process, referred to as the \key{$\epsilon$-machine}  \citep{crutchfield-inferring-1989}.
The \key{statistical complexity} of a process is the state entropy of the $\epsilon$-machine.
Statistical complexity can be computed easily if the $\epsilon$-machine is analytically known, but estimating statistical complexity empirically from time series data is very challenging and seems to at least require additional assumptions about the process~\citep{clarke2003application}.

%There are mor relations between the $\epsilon$-machine and Predictive Rate-Distortion:
%The problem of estimating statistical complexity, and the $\epsilon$-machine, is related to Predictive Rate-Distortion.
\citet{marzen-predictive-2016} show that predictive rate-distortion can be computed when the $\epsilon$-machine is analytically known, by proving that it is equivalent to the problem of compressing causal states, i.e. equivalence classes of pasts, to predict causal states of the backwards process, i.e., equivalence classes of futures.
Further, \cite{still-optimal-2010} show that, in the limit of $\lambda \rightarrow 0$, the code $Z$ that optimizes Predictive Rate-Distortion~(\ref{eq:ib}) turns into the causal states.

%The most generally applicable method for estimating statistical complexity and predictive rate distortion from time series data is Optima Causal Estimation.


\subsection{Related Work}

There are related models that represent past observations by extracting those features that are relevant for prediction.
Predictive State Representations \citep{singh-learning-2003,singh-predictive-2004} and Observable Operator Models \citep{jaeger1998discrete} encode past observations as sets of predictions about future observations.
\citet{rubin2012trading} study agents that trade the cost of representing information about the environment against the reward they can obtain by choosing actions based on the representation.
Relatedly, \citet{still-information-2014} introduces a Recursive Past Future Information Bottleneck where past information is compressed repeatedly, not just at one reference point in time.


As discussed in Section~\ref{sec:complexity}, estimating predictive rate-distortion is related to the problem of estimating statistical complexity.
\citet{clarke2003application} and \citet{still-optimal-2010} consider the problem of estimating statistical complexity from finite data.
While statistical complexity is hard to identify from finite data in general, \citet{clarke2003application} introduce certain restrictions on the underlying process that make this more feasible.


%%It also turned up in \cite{creutzig-past-future-2009}, \cite{still-information-2014}.



\section{Prior Work: Optimal Causal Filtering}\label{sec:OCF}

The main prior method for estimating predictive rate-distortion from data is Optimal Causal Filtering (OCF, \citet{still-optimal-2010}).
This method approximates predictive rate-distortion using two approximations:
First, it replaces semi-infinite pasts and futures with bounded-length contexts, i.e., pairs of finite past contexts $(\finitepast := X_{-M}\dots X_{-1})$ and future contexts $(\finitefuture := X_{0}\dots X_{M-1})$ of some finite length $M$.\footnote{It is not crucial that past and future contexts have the same lengths, and indeed \citet{still-optimal-2010} do not assume this. We do assume equal length throughout this paper for simplicity of our experiments, though nothing depends on this.}
The PRD objective (\ref{eq:ib}) then becomes (\ref{eq:ib-oce}), aiming to predict length-$M$ finite futures from summary codes $Z$ of length-$M$ finite pasts:
\begin{equation}\label{eq:ib-oce}
	\max_{Z : Z \bot \finitefuture | \finitepast} \left( \operatorname{I}[Z, \finitefuture] - \lambda \cdot \operatorname{I}[\finitepast, Z] \right).
\end{equation}
Second, OCF estimates information measures directly from the observed counts of $(\finitepast), (\finitefuture)$ using the plug-in estimator of mutual information. 
With such an estimator, the problem in (\ref{eq:ib-oce}) can be solved using a variant of the Blahut-Arimoto algorithm \citep{tishby-information-1999}, obtaining an encoder $P(Z|\finitepast)$ that maps each observed past sequence $\finitepast$ to a distribution over a (finite) set of code words $Z$.

Two main challenges have been noted in prior work:
First, solving the problem for a finite empirical sample leads to overfitting, overestimating the amount of structure in the process.
\citet{still-optimal-2010} address this by subtracting an asymptotic correction term that becomes valid in the limit of large $M$ and $\lambda \rightarrow 0$, when the codebook $P(Z|\past)$ becomes deterministic, and which allows them to select a deterministic codebook of an appropriate complexity.
This leaves open how to obtain estimates outside of this regime, when the codebook can be far from deterministic.

The second challenge is that OCF requires the construction of a matrix whose rows and columns are indexed by the observed past and future sequences \citep{marzen-predictive-2016}.
Depending on the topological entropy of the process, the number of such sequences can grow as $|A|^M$, where $A$ is the set of observed symbols, and processes of interest often do show this exponential growth \citep{marzen-predictive-2016}.
Drastically, in the case of natural language, $A$ contains thousands of words.
%We will see that this makes applying OCF to language data infeasible even with $M=1$.

A further challenge is that OCF is infeasible if the number of required codewords is too large, again because it requires constructing a matrix whose rows and columns are indexed by the codewords and observed sequences.
Given that storing and manipulating matrices greater than $10^5 \times 10^5$ is currently not feasible, a setting where $\operatorname{I}[\past, Z] > \log 10^5 \approx 11.5$ cannot be captured with OCF.

\section{Neural Estimation via Variational Upper Bound}\label{sec:nprd}

We now introduce our method, Neural Predictive Rate-Distortion (NPRD), to address the limitations of OCF, by using parametric function approximation:
Whereas OCF constructs a codebook mapping between observed sequences and codes, we use general-purpose function approximation estimation methods to compute the representation $Z$ from the past and to estimate a distribution over future sequences from $Z$.
In particular, we will use recurrent neural networks, which are known to provide good models of sequences from a wide range of domains; our method will be applicable to other types of function approximators also.
%The parameters of the function approximators are optimized on the observed pairs of pasts and futures.
%When deploying the resulting mapping on unseen data, the function approximator can be used to generate appropriate representations $Z$ even for sequences that did not occur in the initial data sample.

This will have two main advantages, addressing the limitations of OCF:
First, unlike OCF, function approximators can discover generalizations across similar sequences, enabling the method to calculate good codes $Z$ even for past sequences that were not seen previously.
This is of paramount importance in settings where the state space is large, such as the set of words of a natural language.
Second, the cost of storing and evaluating the function approximators will scale \emph{linearly} with the length of observed sequences both in space and in time, as opposed to the exponential memory demand of OCF.
This is crucial for modeling long dependencies.

\subsection{Main Result: Variational Bound on Predictive Rate-Distortion}
We will first describe the general method, without committing to a specific framework for function approximation yet.
We will construct a bound on predictive rate-distortion and optimize this bound in a parametric family of function approximators to obtain an encoding $Z$ that is close to optimal for the nonparametric objective~(\ref{eq:ib}).

As in OCF (Section~\ref{sec:OCF}), we assume that a set of finite sample trajectories $x_{-M} \dots x_{M-1}$ is available, and we aim to compress pasts of length $M$ to predict futures of length $M$. To carry this out, we restrict the PRD objective~(\ref{eq:ib}) to such finite-length pasts and futures: 
\begin{equation}\label{eq:ib-oce-repeat}
	\max_{Z : Z \bot \finitefuture | \finitepast} \left( \operatorname{I}[Z, \finitefuture] - \lambda \cdot \operatorname{I}[\finitepast, Z] \right).
\end{equation}
%where $Z$ ranges over all random variables such that the Markovcondition~(\ref{eq:markov}) holds:
%\begin{equation}
%    Z\bot \finitefuture | \finitepast
%\end{equation}
It will be convenient to equivalently rewrite (\ref{eq:ib-oce-repeat}) as
\begin{equation}\label{eq:prd-loss}
	\min_{Z : Z \bot \finitefuture | \finitepast}	\left[\operatorname{H}[\finitefuture | Z] + \lambda \cdot\operatorname{I}[ \finitepast, Z]\right],
\end{equation}
where $\operatorname{H}[\finitefuture | Z]$ is the \key{prediction loss}. Note that minimizing prediction loss is equivalent to maximizing predictiveness $\operatorname{I}[\finitefuture , Z]$. % is the reduction in prediction loss when predicting


When deploying such a predictive code $Z$, two components have to be computed: a distribution $P(Z|\finitepast)$ that \key{encodes} past observations into a code $Z$, and a distribution $P(\finitefuture|Z)$ that \key{decodes} the code $Z$ into predictions about the future.

Let us assume that we already have some encoding distribution
\begin{equation}
Z \sim P_\phi(Z|\finitepast),
\end{equation}
where $\phi$ is the encoder, expressed in some family of function approximators. The encoder transduces an observation sequence $\finitepast$ into the parameters of the distribution $P_\phi(\cdot|\finitepast)$.
From this encoding distribution, one can obtain the optimal decoding distribution over future observations via Bayes' rule: %, using the Markov condition: 
\begin{equation}
	\label{eq:cond-fut}
	\begin{split}
	P(\finitefuture|Z) &= \frac{P(\finitefuture, Z)}{P(Z)} = \frac{\E_{\finitepast} P(\finitefuture, Z|\finitepast)}{\E_{\finitepast} P_\phi(Z|\finitepast)} =^{(*)} \frac{\E_{\finitepast}   P(\finitefuture|\finitepast)     P_\phi(Z|\finitepast)}{\E_{\finitepast} P_\phi(Z|\finitepast)} \\
%	&= \frac{\E_{X_{-M \dots M-1}} P_\phi(Z|\finitepast)}{\E_{\finitepast} P_\phi(Z|\finitepast)}
	\end{split}
\end{equation}
where $(*)$ uses the Markov condition $Z \bot \finitefuture | \finitepast$.
However, neither of the two expectations in the last expression of~(\ref{eq:cond-fut}) is tractable, as they require summation over exponentially many sequences, and algorithms (e.g., dynamic programming) to compute this sum efficiently are not available in general.
For a similar reason, the rate $\operatorname{I}[\finitepast, Z]$ of the code $Z \sim P_\phi(Z|\finitepast)$ is also generally intractable.
%If $\phi$ is given by a powerful function approximating family such as recurrent neural networks, 


% That is, we introduce $\psi$ and $q$ as \emph{variational approximations} \citep{blei-variational-2016} to the optimal decoder and the true marginal of $Z$.


Our method will be to introduce additional functions, also expressed using function approximators, that approximate some of these intractable quantities:
First, we will use a parameterized probability distribution $q$ as an approximation to the intractable marginal $P(Z) = \E_{\finitepast} P_\phi(Z|\finitepast)$:
\begin{equation}
q(Z) \text{ approximates } P(Z) = \E_{\finitepast} P_\phi(Z|\finitepast).
\end{equation}
Second, to approximate the decoding distribution $P(\finitefuture|Z)$, we introduce a parameterized decoder $\psi$ that maps points $Z \in \mathbb{R}^N$  into probability distributions $P_\psi(\finitefuture|Z)$ over future observations $\finitefuture$:
\begin{equation}
P_\psi(\finitefuture|Z)  \text{ approximates }  P(\finitefuture|Z)
\end{equation}
for each code $Z \in \mathbb{R}^N$. Crucially, $P_\psi(\finitefuture|Z)$ will be easy to compute efficiently, unlike the exact decoding distribution $P(\finitefuture|Z)$.

If we fix a stochastic process $(X_t)_{t \in \mathbb{Z}}$ and an encoder $\phi$, then the following two bounds hold for \emph{any} choice of the decoder $\psi$ and the distribution $q$:

\begin{proposition}
\label{prop:mi-bound}
The loss incurred when predicting the future from $Z$ via $\psi$ upper-bounds the true conditional entropy of the future given $Z$, when predicting using the exact decoding distribution (\ref{eq:cond-fut}):
\begin{equation}\label{ineq1}
	-	\mathbb{E}_{X}\mathbb{E}_{Z \sim \phi(X)}\left[\log P_\psi(\finitefuture | Z)\right] \geq \operatorname{H}[\finitefuture|Z].
\end{equation}
Furthermore, equality is attained if and only if $P_\psi(\finitefuture|Z) = P(\finitefuture|Z)$. % (as defined in~(\ref{eq:cond-fut})).
\end{proposition}

\begin{proof}
By Gibbs' inequality:
\begin{align*}
	-	\mathbb{E}_{X}\mathbb{E}_{Z \sim \phi(X)}\left[\log P_\psi(\finitefuture | Z)\right] & \geq -	\mathbb{E}_{X}\mathbb{E}_{Z \sim \phi(X)}\left[\log P(\finitefuture | Z)\right]\\
	&= \operatorname{H}[\finitefuture|Z].
\end{align*}
\end{proof}

\begin{proposition} 
\label{prop:kl-bound}
The KL Divergence between $P_\phi(Z|\finitepast)$ and $q(Z)$, averaged over pasts $\finitepast$, upper-bounds the rate of $Z$: % mutual information between $Z$ and the past observations:
\begin{equation}\label{ineq2}
\begin{aligned}
	\mathbb{E}_{\finitepast}\left[ \operatorname{D_{KL}}\infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right] &=   \mathbb{E}_{\finitepast} \mathbb{E}_{Z|\finitepast} \log \frac{P_\phi(Z|\finitepast)}{q(Z)}  \\
	& \geq  \mathbb{E}_{\finitepast} \mathbb{E}_{Z|\finitepast} \log \frac{P_\phi(Z|\finitepast)}{P(Z)}  \\
	& = \operatorname{I}[\finitepast,Z].
\end{aligned}
\end{equation}
Equality is attained if and only if $q(Z)$ is equal to the true marginal $P(Z) = \E_{\finitepast} P_\phi(Z|\finitepast)$.
\end{proposition}

\begin{proof}
The two equalities follow from the definition of KL Divergence and Mutual Information. To show the inequality, we again use Gibbs' inequality:
\begin{align*}
    -\mathbb{E}_{\finitepast} \mathbb{E}_{Z|\finitepast} \log q(Z) = - \mathbb{E}_Z \log q(Z) \geq - \mathbb{E}_Z \log P(Z) = -\mathbb{E}_{\finitepast} \mathbb{E}_{Z|\finitepast} \log P(Z).
\end{align*}
Here, equality holds if and only if $q(Z) = P(Z)$, proving the second assertion.
\end{proof}

We now use the two propositions to rewrite the predictive rate-distortion objective~(\ref{eq:prd-loss}) in a way amenable to using function approximators, which is our main theoretical result, and the foundation of our proposed estimation method:

\begin{corollary}[Main Result]
The predictive rate-distortion objective~(\ref{eq:prd-loss})
\begin{equation}\tag{\ref{eq:prd-loss}}
	\min_{Z : Z \bot \finitefuture | \finitepast}	\left[\operatorname{H}[\finitefuture | Z] + \lambda \operatorname{I}[ \finitepast, Z]\right]
\end{equation} % should be P_\psi (X_{0...M}|z)? % done (mhahn)
is equivalent to
\begin{equation}\label{eq:bound}
	\min_{\phi, \psi, q} \left[\mathbb{E}_{\finitepast, \finitefuture}	\left[-	\mathbb{E}_{Z \sim \phi(\finitepast)}\left[\log P_\psi(\finitefuture | Z)\right] + \lambda \cdot \operatorname{D_{KL}} \left[ \infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right]\right]\right],
\end{equation} % should be P_\psi (X_{0...M}|z)? % done (mhahn)
where $\phi, \psi, q$ range over all triples of the appropriate types described above.

From a solution to (\ref{eq:bound}), one obtains a solution to (\ref{eq:prd-loss}) by setting $Z \sim P_\phi(\cdot|\finitepast)$.
%The second proposition guarantees that, for a solution to  (\ref{eq:prd-loss}), 
The rate of this code is given as follows
\begin{equation}\label{eq:eq1-new}
 \operatorname{I}[Z, \finitepast] =   \mathbb{E}_{\finitepast}\left[ \operatorname{D_{KL}}\infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right]
\end{equation}
and the prediction loss is given by
\begin{equation}\label{eq:eq2-new}
  %H[X_{0\dots M} | Z] =  -	\mathbb{E}_{z \sim \phi(\finitepast)}\left[\log P_\psi(X_{0\dots M} | z)\right]
  \operatorname{H}[\finitefuture|Z] =   - \mathbb{E}_{X_{\finitepast,\finitefuture}}	\mathbb{E}_{Z \sim P_\phi(\finitepast)}\left[\log P_\psi(\finitefuture | Z)\right]. %H[\finitefuture|\finitepast] % \operatorname{H}[\finitefuture]
\end{equation}
\end{corollary}

\begin{proof}
By the two propositions, the term inside the minimization in (\ref{eq:bound}) is an upper bound on (\ref{eq:prd-loss}), and takes on that value if and only if $P_\phi(\cdot|\finitepast)$ equals the distribution of the $Z$ optimizing (\ref{eq:prd-loss}), and $\psi, q$ are as described in those propositions.
\end{proof}

Note that the right-hand-sides of (\ref{eq:eq1-new}-\ref{eq:eq2-new}) can both be estimated efficiently using Monte Carlo samples from $Z \sim P_\phi(\finitepast)$.

If $\phi, \psi, q$ are not exact solutions to (\ref{eq:bound}), the two propositions guarantee that we still have bounds on rate and prediction loss for the code $Z$ generated by $\phi$:
\begin{equation}\label{eq:bound1-new}
 \operatorname{I}[Z, \finitepast] \leq   \mathbb{E}_{\finitepast}\left[ \operatorname{D_{KL}}\infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right]
\end{equation}
\begin{equation}\label{eq:bound2-new}
  %I[X_{-M\dots 0}, X_{0\dots M} | Z] \leq -	\mathbb{E}_{z \sim \phi(\finitepast)}\left[\log P_\psi(X_{0\dots M} | z)\right]- H[\finitefuture|\finitepast]
  \operatorname{H}[\finitefuture|Z] \leq   - \mathbb{E}_{X_{\finitepast, \finitefuture}}	\mathbb{E}_{Z \sim P_\phi(\finitepast)}\left[\log P_\psi(\finitefuture | Z)\right]. % \operatorname{H}[\finitefuture]
\end{equation}
To carry out the optimization~(\ref{eq:bound}), we will restrict $\phi, \psi, q$ to a powerful family of parametric families of function approximators, within which we will optimize the objective with gradient descent.
While the solutions may not be exact solutions to the nonparametric objective~(\ref{eq:bound}), they will still satisfy the bounds (\ref{eq:bound1-new}-\ref{eq:bound2-new}), and---if the family of approximators is sufficiently rich---can come close to turning these into the equalities (\ref{eq:eq1-new}-\ref{eq:eq2-new}).
 



\subsection{Choosing Approximating Families}
For our method of Neural Predictive Rate-Distortion (NPRD), we choose the approximating families for the encoder $\phi$, the decoder $\psi$, and the distribution $q$ to be certain types of neural networks that are known to provide strong and general models of sequences and distributions.

For $\phi$ and $\psi$, we use recurrent neural networks with Long Short Term Memory (LSTM) cells \citep{hochreiter-long-1997}, widely used for modeling sequential data across different domains.
We parameterize the distribution $P_\phi(Z|\finitepast)$ as a Gaussian whose mean and variance are computed from the past $\finitepast$:
We use an LSTM network to compute a vector $h \in \mathbb{R}^k$ from the past observations $\finitepast$, and then compute
\begin{equation}\label{eq:z-gauss}
	Z \sim \mathcal{N}(W_\mu h, (W_\sigma h)^2 I_{k\times k}),
\end{equation}
where $W_\mu, W_\sigma \in \mathbb{R}^{k\times k}$ are parameters.
While we found Gaussians sufficiently flexible for $\phi$, more powerful encoders could be constructed using more flexible parametric families, such as normalizing flows \citep{rezende-variational-2015, kingma-improving-2016}.

For the decoder $\psi$, we use a second LSTM network to compute a sequence of vector representations $g_t = \psi(Z, X_{0\dots t-1})$ ($g_t \in \mathbb{R}^k$) for $t = 0, \dots M-1$.
We compute predictions using the softmax rule 
\begin{equation}
	P_\psi(X_t = s_i|X_{1\dots t-1}, Z) \propto \exp((W_o g_t)_i)
\end{equation}
for each element $s_i$ of the state space $S = \{s_1, ..., s_{|S|}\}$, and $W_o \in \mathbb{R}^{|S| \times k}$ is a parameter matrix to be optimized together with the parameters of the LSTM networks.


For $q$, we choose the family of \key{Neural Autoregressive Flows} \citep{huang-neural-2018}. 
This is a parametric family of distributions that allows efficient estimation of the probability density and its gradients.
This method widely generalizes a family of prior methods \citep{rezende-variational-2015, kingma-improving-2016, papamakarios-masked-2017}, offering efficient estimation while surpassing prior methods in expressivity.


\subsection{Parameter Estimation and Evaluation}\label{sec:parameter-estimation}
We optimize the parameters of the neural networks expressing $\phi, \psi, q$ for the objective (\ref{eq:bound}) using Backpropagation and Adam \citep{kingma-adam:-2014}, a standard and widely used gradient descent-based method for optimizing neural networks.
During optimization, we approximate the gradient by taking a single sample from $Z$ (\ref{eq:z-gauss}) per sample trajectory $X_{-M}, \dots, X_{M-1}$ and use the reparametrized gradient estimator introduced by \citet{kingma-auto-encoding-2014}.
This results in an unbiased estimator of the gradient of (\ref{eq:bound}) w.r.t. the parameters of $\phi, \psi, q$.

Following standard practice in machine learning, we split the data set of sample time series into three partitions (\key{training set}, \key{validation set}, and \key{test set}). 
We use the training set for optimizing the parameters as described above.
After every pass through the training set, the objective (\ref{eq:bound}) is evaluated on the validation set using a Monte Carlo estimate with one sample $Z$ per trajectory; optimization terminates once the value on the validation set does not decrease any more.
%After this, we use the test set to estimate rate and predictiveness of the fully optimized model (see next paragraph).

After optimizing the parameters on a set of observed trajectories, we estimate rate and prediction loss on the test set.
Given parameters for $\phi, \psi, q$, we evaluate the PRD objective~(\ref{eq:bound}), rate (\ref{eq:bound1-new}), and the prediction loss (\ref{eq:bound2-new}) on the test set by taking, for each time series $X_{-M}...X_{M-1} = \finitepast\finitefuture$ from the test set, a single sample $z  \sim \mathcal{N}(\mu, \sigma^2)$ and computing Monte Carlo estimates for rate
\begin{equation}\label{eq:bound-mc-rate}
\mathbb{E}_{X}\left[ \operatorname{D_{KL}}\infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right] \approx	\frac{1}{N}	\sum_{X_{-M...M} \in TestData}  \log \frac{p_{\mathcal{N}(\mu, \sigma^2)}(z)}{q(z)},
\end{equation}
where $p_{\mathcal{N}(\mu, \sigma^2)}(z)$ is the Gaussian density with $\mu, \sigma^2$ computed from $\finitepast$ as in (\ref{eq:z-gauss}), and prediction loss 
\begin{equation}\label{eq:bound-mc-distortion}
 -	\mathbb{E}_{Z \sim \phi(\finitepast)}\left[\log P_\psi(\finitefuture | Z)\right] \approx - \frac{1}{N}	\sum_{X_{-M...M} \in TestData}	\log P_\psi(\finitefuture | Z).
\end{equation}
%and the PRD objective
%\begin{equation}\label{eq:bound-mc}
%	\frac{1}{N}	\sum_{X_{-M...M} \in TestData}	\log P(X_{0\dots M} | z_{-M...-1}) + \lambda \cdot \log \frac{\mathcal{N}(\mu, \sigma^2)(z)}{q(z)}
%\end{equation}
%Note that one can go from distortion to predictiveness by estimating
Thanks to (\ref{eq:bound1-new}-\ref{eq:bound2-new}), these estimates are guaranteed to be \emph{upper bounds} on the true rate and prediction loss achieved by the code $Z  \sim \mathcal{N}(\mu, \sigma^2)$, up to sampling error introduced into the Monte Carlo estimation by sampling $z$ and the finite size of the test set.


%The resulting value of (\ref{eq:bound-mc}) is guaranteed to be an \emph{upper bound} on the true objective (\ref{eq:prd-recall}), up to sampling error introduced by sampling $z$ and the finite size of the test set.
%When evaluating (\ref{eq:bound}) on unseen data this way, the resulting value is guaranteed to be an \emph{upper} bound on the true predictive rate-distortion due to (\ref{ineq1}-\ref{ineq2}), up to sampling error introduced by sampling $z$ and the finite size of the held-out dataset.

It is important to note that this sampling error is different from the overfitting issue affecting OCF: 
Our Equations (\ref{eq:bound-mc-rate}-\ref{eq:bound-mc-distortion}) provide \emph{unbiased} estimators of upper bounds, whereas overfitting \emph{biases} the values obtained by OCF.
Given that NPRD provably provides upper bounds on rate and prediction loss (up to sampling error), one can objectively compare the quality of different estimation methods:
Among methods that provide upper bounds, the one that provides the lowest such bound for a given problem is the one giving results closest to the true curve.

\subsubsection{Estimating Predictiveness}

Given the estimate for prediction loss, we estimate predictiveness $\operatorname{I}[Z, \finitefuture]$ with the following method.
%by estimating $\operatorname{H}[\finitefuture]$ 
We use the encoder network that computes the code vector $h$ (\ref{eq:z-gauss}) to also estimate the marginal probability of the past observation sequence, $P_\eta(\finitepast)$. %\rljf{How?}
Similar to the decoder $\psi$, we use the vector representations $f_t \in \mathbb{R}^k$ computed by the LSTM encoder after processing $X_{-M\dots t}$ for $t = -M, \dots, -1$, and then compute predictions using the softmax rule
\begin{equation}
	P_\eta(X_t = s_i|X_{1\dots t-1}, Z) \propto \exp((W_{o'} f_t)_i)
\end{equation}
where $W_{o'} \in \mathbb{R}^{|S|\times k}$ is another parameter matrix.

Because we consider stationary processes, we have 
that the cross-entropy under $P_\eta$ of $\finitefuture$ is equal to the cross-entropy of $\finitepast$ under the same encoding distribution: $\mathbb{E}_{X_{\finitepast\finitefuture}}\left[\log P_\eta(\finitefuture)\right] = - \mathbb{E}_{X_{\finitepast\finitefuture}}\left[\log P_\eta(\finitepast)\right]$. \rljf{I see that this is true if $P_\eta$ is the true marginal but what if it is approximate? Could $P_\eta$ end up non-stationary even when it comes from an encoder fit to a stationary distribution? Or is the stationarity of $P_\eta$ guaranteed because the encoder LSTM is autoregressive?} \mhahn{If $X_t$ is stationary, then the distributions of $\finitepast$ and $\finitefuture$ are the same, so these expectations are the same. Is there a way to make this more transparent?}
Using this observation, we estimate the predictiveness $\operatorname{I}[Z, \finitefuture] = \operatorname{H}[\finitefuture] - \operatorname{H}[\finitefuture|Z]$ by the difference between the corresponding cross-entropies on the test set~\cite{mcallester2018formal}:
%$\operatorname{H}[\finitefuture]$ by the cross-entropy of $P_\eta$ on the test data:
\begin{equation}\label{eq:bound-mc-predictiveness}
- \mathbb{E}_{X_{\finitepast\finitefuture}}\left[\log P_\eta(\finitefuture) - \log P_\psi(\finitefuture | Z)\right],
\end{equation}
which we approximate using Monte-Carlo sampling on the test set as in~(\ref{eq:bound-mc-rate}-\ref{eq:bound-mc-distortion}).

During optimization, we add the cross-entropy term $- \mathbb{E}_{X_{\finitepast\finitefuture}}\left[\log P_\eta(\finitepast)\right]$ to the PRD objective~(\ref{eq:bound}) \rljf{Why?}, so that the full training objective comes out to:
\begin{equation}\label{eq:bound-augmented}
	\min_{\phi, \psi, q,\eta} \left[\mathbb{E}_{\finitepast, \finitefuture} \left[ -	\mathbb{E}_{Z \sim \phi(\finitepast)}\left[\log P_\psi(\finitefuture | Z)\right] + \lambda \cdot \operatorname{D_{KL}} \left[ \infdivx{P_\phi(Z|\finitepast)}{q(Z)}\right] - \log P_\eta(\finitepast) \right]\right].
\end{equation} % should be P_\psi (X_{0...M}|z)? % done (mhahn)
Again by Gibbs' inequality and Propositions~\ref{prop:mi-bound} and~\ref{prop:kl-bound}, this is minimized when $P_\eta$ represents the true distribution over length-$M$ blocks $P(\finitepast)$, $P_\phi(Z|\finitepast)$ describes an optimal code for the given $\lambda$, $q$ is its marginal distribution, and $P_\psi(\finitefuture|Z)$ is the Bayes-optimal decoder.
For approximate solutions to this augmented objective, the inequalities~(\ref{eq:bound1-new}-\ref{eq:bound2-new}) will also remain true due to Propositions~\ref{prop:mi-bound} and~\ref{prop:kl-bound}.

%The magnitude of the estimation error could be estimated using standard methods such as bootstrapping, cross-validation, and taking more samples of $z$---



%\begin{equation}\label{eq:bound1-new}
% I[Z, \finitepast] \leq   \mathbb{E}_{X}\left[ \operatorname{D_{KL}}\infdivx{P(Z|X)}{q(Z)}\right]
%\end{equation}
%\begin{equation}\label{eq:bound2-new}
%  H[X_{0\dots M} | Z] \leq -	\mathbb{E}_{z \sim \phi(\finitepast)}\left[\log P_\psi(X_{0\dots M} | z)\right]
%\end{equation}



\subsection{Related Work}
In (\ref{eq:bound}), we derived a variational formulation of predictive rate-distortion.
This is formally related to a variational formulation of the Information Bottleneck that was introduced by \cite{alemi-deep-2016}, who applied it to neural-network based image recognition.
Unlike our approach, they used a fixed diagonal Gaussian instead of a flexible parametrized distribution for $q$.
%As far as we are aware, no previous work has used such a variational formulation specifically .
Some recent work has applied similar approaches to the modeling of sequences, employing models corresponding to the objective~(\ref{eq:bound}) with $\lambda=1$~\citep{grathwohl2016disentangling,walker2016uncertain,fraccaro2017disentangled,hernandez2018variational}.

In the neural networks literature, the most commonly used method using variational bounds similar to Equation~(\ref{eq:bound}) is the Variational Autoencoder \citep{kingma-auto-encoding-2014, bowman-generating-2016}, which corresponds to the setting where $\lambda=1$ and the predicted output is equal to the observed input.
The $\beta$-VAE \citep{higgins2017beta}, a variant of the Variational Autoencoder, uses $\lambda > 1$ (whereas the Predictive Rate-Distortion objective~(\ref{eq:ib}) uses $\lambda \in (0,1)$), and has been linked to the Information Bottleneck by \cite{burgess2018understanding}.\footnote{As discussed in Section~\ref{sec:background}, our model becomes degenerate with $\lambda \ge 1$. Surprisingly, $\beta$-VAE operates in exactly this parameter regime, although }



%\subsection{Discusion}


\section{Experiments}\label{sec:experiments}
%In Section~\ref{sec:nprd}, we described our method Neural Predictive Rate Distortion (NPRD) 
We now test the ability of our new method NPRD to estimate rate-distortion curves.
Before we apply NPRD to obtain the first estimates of predictive rate-distortion for natural language in Section~\ref{sec:language}, we validate the method on processes whose tradeoff curves are analytically known, and compare with OCF.

\subsection{Implementation Details}


\paragraph{OCF} As discussed in Section~\ref{sec:OCF}, OCF is affected by overfitting, 
and will systematically overestimate the predictiveness achieved at a given rate~\citep{still-optimal-2010}.
To address this problem, we follow the evaluation method used for NPRD, evaluating rate and predictiveness on held-out test data.
We partition the available time series data into a training and test set. We use the training set to create the encoder $P(Z|\finitepast)$ using the Blahut-Arimoto algorithm as described by~\citet{still-optimal-2010}.
We then use the held-out test set to estimate rate and prediction loss.
This method not only enables fair comparison between OCF and NPRD, but also provides a more realistic evaluation, by focusing on the performance of the code $Z$ when deployed on new data samples.
For rate, we use the same variational bound that we use for NPRD, stated in Proposition~\ref{prop:kl-bound}:
\begin{equation}\label{eq:rate-OCF}
\frac{1}{N} \sum_{\finitepast \in \text{Test Data}} \operatorname{D_{KL}}\infdivx{P(Z|\finitepast)}{s(Z)}
\end{equation}
where $P(Z|\finitepast)$ is the encoder created by the Blahut-Arimoto algorithm, and $s(Z)$ is the marginal distribution of $Z$ on the training set.
$N$ is the number of sample time series in the test data.
In the limit of enough training data, when $s(Z)$ matches the actual population marginal of $Z$, (\ref{eq:rate-OCF}) is an unbiased estimate of the rate. %  If the training set is large enough and 
%\textbf{Is $s$ defined anywhere? same for $r$?}
We estimate the prediction loss on the future observations as the empirical cross-entropy, i.e., the variational bound stated in Proposition~\ref{prop:mi-bound}:
\begin{equation}
\frac{1}{N} \sum_{\finitepast, \finitefuture \in \text{Test Data}} \E_{Z \sim P(\cdot|\finitepast)} \log P(\finitefuture|Z).
\end{equation}
where $P(\finitefuture|Z)$ is the decoder obtained from the Blahut-Arimoto algorithm on the training set.
Thanks to Propositions~\ref{prop:mi-bound} and \ref{prop:kl-bound}, these quantities provide upper bounds, up to sampling error introduced by finiteness of the held-out data.
Again, sampling error does not bias the results in either direction, unlike overfitting, which introduces a systematic bias.


Held-out test data may contain sequences that did not occur in the training data. Therefore, we add a pseudo-sequence $\omega$ and add pseudo-observations $(\omega, \finitefuture)$, $(\finitepast, \omega)$, $(\omega, \omega)$ for all observed sequences $\finitepast\finitefuture$ to the matrix of observed counts that serves as the input to the Blahut-Arimoto algorithm.
These pseudo-observations were assigned pseudo-counts $\gamma$ in this matrix of observed counts; we found that a wide range of values ranging from $0.0001$ to $1.0$ yielded essentially the same results.
When evaluating the codebook on held-out data, previously unseen sequences were mapped to $\omega$.


\paragraph{NPRD}
For all experiments, we used $M=15$.
Neural networks have hyperparameters, such as the number of units and the step size used in optimization, which affect the quality of approximation depending on properties of the dataset and the function being approximated.
Given that NPRD provably provides upper bounds on the PRD objective~(\ref{eq:prd-loss}), one can in principle identify the best hyperparameters for a given process by choosing the combination that leads to the lowest estimated upper bounds.
As a computationally more efficient method, we defined a range of plausible hyperparameters based both on experience reported in the literature, and considerations of computational efficiency.
These parameters are discussed in Appendix~\ref{sec:app-hyperparams}.
We then randomly sampled, for each of the processes that we experimented on, combinations of $\lambda$ and these hyperparameters to run NPRD on.
%We optimize all parameters using Adam~\citep{kingma-adam:-2014}, a variant of stochastic gradient descent~\citep{robbins1951stochastic} commonly used for optimizing neural networks.
We implemented the model using PyTorch \citep{paszke2017automatic}.

\subsection{Analytically Tractable Problems}\label{sec:tractable}

We first test NPRD on two processes where the predictive rate-distortion tradeoff is analytically tractable. 
The \key{Even Process} \citep{marzen-predictive-2016} is the process of 0/1 IID coin flips, conditioned on all blocks of consecutive ones having even length.
Its complexity and excess entropy are both $\approx 0.63$ nats. % (stationary distribution: 1/3 for odd, 2/3 for even)
It has infinite Markov order, and \citet{marzen-predictive-2016} find that OCF (at $M=5$) performs poorly. %, overestimating  even at $M=5$.
The true predictive rate-distortion curve was computed in \cite{marzen-predictive-2016} using the analytically known $\epsilon$-machine.
The \key{Random Insertion Process} \citep{marzen-predictive-2016} consists of sequences of uniform coin flips $X_t \in \{0,1\}$, subject to the constraint that, if $X_{t-2}$ was a 0, then $X_{t}$ has to be a 1.



We applied NPRD to these processes by training on 3M random trajectories of length 30, and using 3000 additional trajectories for validation and test data.
For each process, we ran NPRD 1000 times for random choices of $\lambda \in [0,1]$.
Due to computational constraints, when running OCF, we limited sample size to 3000 trajectories for estimation and as held-out data.
Following \citet{marzen-predictive-2016}, we ran OCF for $M=1,...,5$.

The resulting estimates are shown in Figure~\ref{fig:even}, together with the analytical rate-distortion curves computed by \citet{marzen-predictive-2016}.
Individual runs of NPRD show variation (red dots), but most runs lead to results close to the analytical curve (gray line), and strongly surpass the curves computed by OCF at $M=5$.
Bounding the tradeoff curve using the sets of runs of NPRD results in a close fit (red line) to the analytical tradeoff curves.

\begin{figure*}
\centering
\includegraphics[width=0.45\textwidth]{code/figures/even-info.pdf}
\includegraphics[width=0.45\textwidth]{code/figures/rip-info.pdf}

%\includegraphics[width=0.45\textwidth]{code/figures/even-beta-mem.pdf}
	\caption{Rate-Distortion for the Even Process (left) and the Random Insertion Process (right). Gray lines: Analytical curves. Red dots: Multiple runs of NPRD. Red line: Tradeoff curve computed from NPRD runs. Blue: OCF for $M\leq 5$. }\label{fig:even}
\end{figure*}

\paragraph{Recovering Causal States}
Does NPRD lead to interpretable codes $Z$?
To answer this, we further investigated the NPRD approximation to the Random Insertion Process (RIP), obtained in the previous paragraph.
The $\epsilon$-machine was computed by  \citet{marzen-predictive-2016} and is given in Figure~\ref{fig:rip-machine} (left).
The process has three causal states:
State \textit{A} represents those pasts where the future starts with $1^{2k}0$ ($k = 0, 1, 2, \dots$) -- these are the pasts ending in either $001$ or $10111^m$ ($m= 0, 1, 2, \dots$). % but cannot go in with 10
State \textit{B} represents those pasts ending in 10---the future has to start with 01 or 11.
State \textit{C} represents those pasts ending in either 00 or 01---the future has to start with $1^{2k+1}0$ ($k = 0, 1, 2, \dots$).

The analytical solution to the predictive rate-distortion problem was computed by \citet{marzen-predictive-2016}.
At $\lambda > 0.5$, the optimal solution collapses $A$ and $B$ into a single codeword, while all three states are mapped to separate codewords for $\lambda \leq 0.5$.


Does NPRD recover this picture?
We applied PCA to samples from $Z$ computed at two different values of $\lambda$, $\lambda = 0.25$ and $\lambda = 0.6$. % corresponding to the two points marked in the rate-distortion figure (\textbf{TODO do this}).
The first two principal components of $Z$ are shown in Figure~\ref{fig:latent}.
Samples are colored by the causal states corresponding to the pasts of the trajectories that were encoded into the respective points by NPRD.
On the left, obtained at $\lambda=0.6$, the states A and B are collapsed, as expected.
On the right, obtained at $\lambda=0.25$, the three causal states are reflected as distinct modes of $Z$.
Note that, at finite $M$, a fraction of pasts is ambiguous between the green and blue causal states; these are colored in black and NPRD maps them into a region between the modes corresponding to these states.

In Figure~\ref{fig:rip-machine} (right), we record, for each of the three modes to which cluster the distribution of the code $Z$ shifts when a symbol is appended.
We restrict to those strings that have nonzero probability for RIP (no code will ever be needed for other strings).
For comparison, we show the $\epsilon$-machine computed by \citet{marzen-predictive-2016}.
Comparing the two diagrams shows that NPRD effectively recovers the $\epsilon$-machine:
The three causal states are represented by the three different modes of $Z$, and the effect of appending a symbol also mirrors the state transitions of the $\epsilon$-machine.

\begin{figure*}
\centering
\includegraphics[width=0.45\textwidth]{code/figures/foo_pca_2.png}
\includegraphics[width=0.45\textwidth]{code/figures/foo_pca_3.png}
	\caption{Applying Principal Component Analysis to 5000 sampled codes $Z$ for the Random Insertion Process, at $\lambda = 0.6$ (left) and $\lambda = 0.25$ (right). We show the first two principal components. Samples are colored according to the states in the $\epsilon$-machine. There is a small number of samples from sequences that, at $M=15$, cannot be uniquely attributed to any of the states (ambiguous between A and C); these are indicated in black.}\label{fig:latent}
\end{figure*}
% colors = [{"A" : "red", "B" : "green", "C" : "blue", "U" : "black"}[x] for x in states]

\begin{figure*}
\centering
\begin{tikzpicture}[shorten >=1pt,node distance=4cm,on grid,auto] 
   \node[state, fill=green] (B)   {$B$}; 
   \node[state, fill=red] (A) [below left=of B] {$A$}; 
   \node[state, fill=blue] (C) [below right=of B] {$C$}; 
    \path[->] 
	(A) edge [bend left=20] node [above] {$\frac{1}{2} | 0$} (B)
	(A) edge [bend left=20] node [below] {$\frac{1}{2} | 1$} (C)
	(B) edge [bend left=20] node [above] {$\frac{1}{2} | 0$} (C)
	(B) edge [bend right=20] node [left] {$\frac{1}{2} | 1$} (C)
	(C) edge [bend left=20] node [left] {$1 | 1$} (A)
	;
%
%%          edge  node [swap] {1} (B)
%    (C) edge  node  {1} (A)
%          edge [loop above] node {0} ()
%	(A) edge  node [swap] {0} (C) 
%          edge [loop below] node {1} ();
\end{tikzpicture}
\includegraphics[width=0.45\textwidth]{code/figures/foo_pca_3_machine.png}
	\caption{Recovering the $\epsilon$-machine from NPRD. Left: The $\epsilon$-machine of the Random Insertion Process, as described by~\cite{marzen-predictive-2016}. Right: After computing a code $Z$ from a past $x_{-15\dots-1}$, we recorded which of the three clusters the code moves to when appending the symbol $0$ or $1$ to the past sequence. The resulting transitions mirror those in the $\epsilon$-machine. }\label{fig:rip-machine}
\end{figure*}






%- p. 14 in https://arxiv.org/pdf/1412.2859.pdf

%\paragraph{RRXOR}
%
%
%The process of pairs ABC where $C = A xor B$.
%Stationary.
%We additionally interspersed blocks of neutral letters with geometric length.
%
%36 causal states
%
%%As   an   example,   consider   the   random-random   XOR
%%
%%RRXOR
%%
%%process  that  consists  of  two  successive  random
%%symbols  chosen  to  be  0  or  1  with  equal  probability  and  a
%%third symbol that is the logical exclusive-OR
%%
%%XOR
%%
%%of the
%%two previous. The RRXOR process can be represented by a
%%hidden  Markov  chain  with  five  recurrent  causal  states,  but
%%having a much larger total number of causal states. There are
%%36 causal states, most
%%
%%31
%%
%%of which describe a complicated
%%transient  structure.
%%27
%%As  such,  it  is  a  structurally  complex
%%process  that  an  analyst  may  wish  to  approximate  with  a
%%smaller set of states.
%
%%          for _ in range(10000 if partition == "dev" else 100000):
%%              a = (random.random() > 0.5)
%%              b = (random.random() > 0.5)
%%
%%              while random.random() > 0.9: # randomness makes it stationary
%%                  data[-1].append(encode("2"))
%%              data[-1].append(encode("1" if a else "0"))
%%              while random.random() > 0.9:
%%                  data[-1].append(encode("2"))
%%              data[-1].append(encode("1" if b else "0"))
%%              while random.random() > 0.9:
%%                  data[-1].append(encode("2"))
%%              data[-1].append(encode("1" if a != b else "0"))
%%
%
%
%
%\paragraph{Process with Low Excess Entropy and High Memory}
%
%the one we talked about recently (TODO)
%
%

\paragraph{A Process with Many Causal States}
We have seen that NPRD recovers the correct tradeoff, and the structure of the causal states, in processes with a small number of causal states.
How does it behave when the number of causal states is very large?
In particular, is it capable of extrapolating to causal states that were never seen during training?

We consider the following process, which we will call \key{\textsc{Copy3}}: $X_{-15}, ..., X_{-1}$ are independent uniform draws from $\{1,2,3\}$, and $X_1 = X_{-1}, ..., X_{15} = X_{-15}$.
This process deviates a bit from our usual setup since we defined it only for $t \in \{-15, ..., 15\}$, but it is well-suited to investigating this question:
The number of causal states is $3^{15} \approx 14$ million.
With exactly the same setup as for the \textsc{Even} and \textsc{RIP} processes, NPRD achieved essentially zero distortion on unseen data, even though the number of training samples (3 Million) was far lower than the number of distinct causal states.
However, we found that, at this setup, NPRD overestimated the rate.
Increasing the number of training samples from 3M to 6M, NPRD recovered codebooks that achieved both almost zero distortion and almost optimal rate, on fresh samples (Figure~\ref{fig:repeat}).
Even then, the number of distinct causal states is more than twice the number of training samples.
These results demonstrate that, by using function approximation, NPRD is capable of extrapolating to unseen causal states, encoding and decoding appropriate codes on the fly.


Note that one could easily design an optimal decoder and encoder for \textsc{Copy3} by hand---the point of this experiment is to demonstrate that NPRD is capable of inducing such a codebook purely from data, in a general-purpose, off-the-shelf manner.
This contrasts with OCF:
Without optimizations specific to the task at hand, a direct application of OCF would require brute-force storing of all 14 million distinct pasts and futures.



\begin{figure*}
%\includegraphics[width=0.45\textwidth]{code/figures/repeat2-ee-mem.pdf}
\centering
\includegraphics[width=0.45\textwidth]{code/figures/repeat3-info.pdf}
	\caption{Rate-Distortion for the \textsc{Copy3} process. We show NPRD samples, and the resulting upper bound in red. The gray line represents the anaytical curve.}\label{fig:repeat}
\end{figure*}



\section{Estimating Predictive Rate-Distortion   for Natural Language}\label{sec:language}

We consider the problem of estimating rate-distortion for natural language.
Natural language has been a testing ground for information-theoretic ideas since Shannon's work.
Much interest has been devoted to estimating the entropy rate of natural language~\citep{shannon1951prediction,takahira2016entropy,bentz2017entropy,takahashi2018cross}.
Indeed, the information density of language has been linked to human processing effort and to language structure.
The word-by-word information content has been shown to impact human processing effort as measured both by per-word reading times~\citep{hale-probabilistic-2001,levy-expectation-based-2008,smith-effect-2013} and by brain signals measured through EEG~\citep{DBLP:conf/acl/FrankOGV13, kuperberg2016we}.
Consequently, prediction is a fundamental component across theories of human language processing  \citep{kuperberg2016we}.
Relatedly, the Uniform Information Density and Constant Entropy Rate hypotheses~\citep{fenk1980konstanz,genzel2002entropy,jaeger2007speakers} state that languages order information in sentences and discourse so that the entropy rate stays approximately constant. % , ferrer2013constant

The relevance of prediction to human language processing makes the \emph{difficulty} of prediction another interesting aspect of language complexity:
Predictive Rate-Distortion describes how much memory of the past humans need to maintain to predict future words accuractely.
Beyond the entropy rate, it thus forms another important aspect of linguistic complexity.


Understanding the complexity of prediction in language holds promise to enabling a deeper understanding of the nature of language as a stochastic process, and to human language processing.
Long-range correlations in text have been a subject of study for a while \citep{schenkel1993long,ebeling1994entropy,ebeling1995long,altmann2012origin,yang2016long,chen2018quantifying}.
Recently, \citet{dkebowski2018natural} has studied the excess entropy of language across long-range discourses, aiming to better understand the nature of the stochastic processes underlying language.
\citet{koplenig2017statistical} shows a link between traditional linguistic notions of grammatical structure and the information contained in word forms and word order.
The idea that predicting future words creates a need to represent the past well also forms a cornerstone of theories of how humans process sentences \citep{gibson-linguistic-1998,futrell-noisy-context-2017}.


%Human do prediction \cite{Bar:2007,Clark:2016}
%There is the related question of how much memory is needed to predict.
%This is captured by NPRD: human memory vs surprisal
%In psycholinguistics, memory known to impact processing.

We study prediction at the range of the words in individual sentences.
As in the previous experiments, we limit our computations to sequences of length 30, already improving over OCF by an order of magnitude.
One motivation is that, when directly estimating PRD, computational cost has to increase with the length of sequences considered, making the consideration of sequences of hundreds or thousands of words computationally infeasible.
Another motivation for this is that we are ultimately interested in predictive rate-distortion as a model of memory in human processing of grammatical structure, formalizing psycholinguistic models of how humans process individual sentences \citep{gibson-linguistic-1998,futrell-noisy-context-2017}, and linking to studies of the relation between information theory and grammar \citep{koplenig2017statistical}.

%Fenk-Oczlon, G. Familiarity, information flow, and linguistic form. In Frequency and the Emergence of Linguistic Structure; Bybee, J.L., Hopper, P.J., Eds.; John Benjamins: Amsterdam, The Netherlands, 2001; pp. 431448.




\subsection{Part-of-speech-level language modeling}\label{sec:pos}

We first consider the problem of predicting English on the level of Part-of-Speech (POS) tags, using the Universal POS tagset \citep{petrov-universal-2012}. 
This is a simplified setting where the vocabulary is small (20 word types), and one can hope that OCF will produce reasonable results.
We use the English portions of the Universal Dependencies Project~\citep{nivre-universal-2017} tagged with Universal POS Tags \citep{petrov-universal-2012}, consisting of approximately 586K words.
We used the training portions to estimate NPRD and OCF, and the validation portions to estimate the rate-distortion curve.
We used NPRD to generate 350 codebooks for values of $\lambda$ sampled from [0,0.4].
We were only able to run OCF for $M \leq 3$, as the number of sequences exceeds $10^4$ already at $M=4$.


The PRD curve is shown in Figure~\ref{fig:eng-pos} (left).
In the setting of low rate and high distortion, NPRD and OCF (blue, $M=1,2,3$) show essentially identical results.
This holds true until $\operatorname{I}[Z, \future] \approx 0.7$, at which point the bounds provided by OCF deteriorate, showing the effects of overfitting.
NPRD continues to provide estimates at greater rates.

Figure~\ref{fig:eng-pos} (center) shows rate as a function of $\log \frac{1}{\lambda}$.
Recall that $\lambda$ is the tradeoff-parameter from the objective function~(\ref{eq:ib}).
In Figure~\ref{fig:eng-pos} (right), we show rate and the mutual information with the future, as a function of $\log \frac{1}{\lambda}$.
As $\lambda \rightarrow 0$, NPRD (red, $M=15$) continues to discover structure, while OCF (blue, plotted for $M=1,2,3$) exhausts its capacity.

Note that NPRD reports rates of 15 nats and more when modeling with very low distortion.
A discrete codebook would need over 3 million distinct codewords for a code of such a rate, exceeding the size of the training corpus (about 500K words), replicating what we found for the \textsc{Copy3} process:
Neural encoders and decoders can use the geometric structure of the code space to encode generalizations across different dimensions, supporting a very large effective number of distinct possible codes.
Unlike discrete codebooks, the geometric structure makes it possible for neural encoders to construct appropriate codes `on the fly' on new input.



\begin{figure*}
\includegraphics[width=0.3\textwidth]{code/figures/english-info.pdf}
%\includegraphics[width=0.45\textwidth]{code/figures/english-beta-mem.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/english-nlogbeta-mem.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/english-nlogbeta-ee.pdf}
	\caption{Left: Rate-Predictiveness for English POS modeling. Center and right: Rate (center) and Predictiveness (right) on English POS Modeling, as a function of $-\log \lambda$. As $\lambda \rightarrow 0$, NPRD (red, $M=15$) continues to discover structure, while OCF (blue, plotted for $M=1,2,3$) exhausts its capacity. }\label{fig:eng-pos}
\end{figure*}



\subsection{Discussion}
Let us now consider the curves in Figure~\ref{fig:eng-pos} in more detail.
Fitting parametric curves to the empirical PRD curves in Figure~\ref{fig:eng-pos}, we find a surprising result that the statistical complexity of English sentences at the POS level appears to be unbounded.

The rate-predictiveness curve (left) shows that, at low rates, predictiveness is approximately proportional to the rate.
At greater degrees of predictiveness, rate grows faster and faster, whereas predictiveness seems to asymptote to $\approx 1.1$ nats.
The asymptote of predictiveness can be identified with the mutual information between past and future observations, $E_0 := \operatorname{I}[\finitepast, \finitefuture]$, which is a lower bound on the excess entropy. The rate should asymptote to the statistical complexity.
Judging by the curve, natural language---at the time scale we are measuring in this experiment---has a statistical complexity much higher than its excess entropy:
At the highest rate measured by NPRD in our experiment, rate is about 20 nats, whereas predictiveness is about 1.1 nats.
If these values are correct, then---due to the convexity of the rate-predictivity curve---statistical complexity exceeds the excess entropy by a factor of at least $\frac{20}{1.1}$. 
Note that this picture agrees qualitatively with the OCF results, which suggest a lower-bound on the ratio of at least $\frac{2.5}{0.6} > 5$.

Now turning to the other plots in Figure~\ref{fig:eng-pos}, we observe that rate increases at least linearly with $\log\frac{1}{\lambda}$, whereas predictiveness again asymptotes.
This is in qualitative agreement with the picture gained from the rate-predictiveness curve.


Let us consider this more quantitatively.
Based on Figure~\ref{fig:eng-pos} (center), we make the ansatz that the map from $\log\frac{1}{\lambda}$ to the rate $R := \operatorname{I}[\finitepast, Z]$ is superlinear:
\begin{equation}\label{eq:r-alpha-beta}
	R = \alpha \left(\log\frac{1}{\lambda}\right)^\beta,
\end{equation}
	with $\alpha>0, \beta>1$.
We fitted $R \approx \left(\log\frac{1}{\lambda}\right)^{1.7}$ ($\alpha=1$, $\beta=1.7$).
Equivalently
\begin{equation}
\frac{1}{\lambda} = \exp\left(\frac{1}{\alpha^{1/\beta}} R^{1/\beta}\right).
\end{equation}
From this, we can derive expressions for rate $R := \operatorname{I}[\finitepast, Z]$ and predictiveness $P := \operatorname{I}[Z, \finitefuture]$ as follows.
For the solution of Predictive Rate-Distortion~(\ref{eq:ib-oce-repeat}), we have
\begin{equation}
	\frac{\partial P}{\partial \theta} - \lambda \frac{\partial R}{\partial \theta} = 0,
\end{equation}
where $\theta$ is the codebook defining the encoding distribution $P(Z|\finitepast)$, and thus
\begin{equation}
\lambda =	\frac{\partial P}{\partial R}.
\end{equation}
Our ansatz therefore leads to the equation
\begin{equation}\label{eq:derivatives}
\frac{\partial P}{\partial R} = \exp\left(-\frac{1}{\alpha^{1/\beta}} R^{1/\beta}\right). %&\ \ \ \partial R/\partial P = \exp\left(\frac{1}{\alpha^{1/\beta}} R^{1/\beta}\right)
\end{equation}
Qualitatively, this says that predictiveness $P$ asymptotes to a finite value, whereas rate $R$---which should asymptote to the statistical complexity---is unbounded. 

\begin{figure*}
\includegraphics[width=0.3\textwidth]{code/figures/english-info-fitted.pdf}
%\includegraphics[width=0.45\textwidth]{code/figures/english-beta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/english-logbeta-mem-fitted.pdf}
	\includegraphics[width=0.3\textwidth]{code/figures/english-nlogbeta-ee-fitted.pdf}
	\caption{Interpolated values for POS-level prediction of English (compare Figure~\ref{fig:eng-pos}) }\label{fig:eng-pos-fitted}
\end{figure*}


Equation~(\ref{eq:derivatives}) has the solution
\begin{equation}\label{eq:sol}
	P = C - \alpha\beta \cdot \Gamma\left(\beta, (R/\alpha)^{1/\beta}\right)
\end{equation}
where $\Gamma$ is the incomplete Gamma function.
Since $\lim_{R \rightarrow \infty} P = C$, the constant $C$ has to equal the maximally possible predictiveness $E_0 := \operatorname{I}[\finitepast, \finitefuture]$.

%As the window size $M \rightarrow \infty$, (\ref{eq:sol}) behaves as
%\begin{equation}
%	P \approx E_0 - \alpha\beta \cdot (R/\alpha)^{\frac{\beta-1}{\beta}} e^{-(R/\alpha)^{1/\beta}},
%\end{equation}
%which asymptotes to $E_0$.
Given the values fitted above ($\alpha=1$, $\beta=1.7$), we found that $E_0 = 1.13$ yielded a good fit.
Using~(\ref{eq:r-alpha-beta}), this can be extended without further parameters to the third curve in Figure~\ref{fig:eng-pos}.
Resulting fits are shown in Figure~\ref{fig:eng-pos-fitted}.
%$E = E_0 - \Gamma(1.6, (\log\beta))$

Note that there are other possible ways of fitting these curves; we have described a simple one that requires only two parameters $\alpha >0$, $\beta > 1$, in addition to a guess for the maximal predictiveness $E_0$.
In any case, the results show that natural language shows an approximately linear growth of predictiveness with rate at small rates, and exploding rates at diminishing returns in predictiveness later.

%Why might language show such behavior?
%One natural idea is that the causal states of natural language are the singletons $\past$; that is, no two past trajectories induce a fully equivalent distribution over future trajectories.
%Then the code $Z^{(T)}$ which stores the block $X_{-T...-1}$ has rate $R_T = \Theta(T)$, but its predictiveness $P_T =	\operatorname{I}[X_{-T...-1}, X_{0,...M-1}]$ asymptotes to $E_0$.

%By writing $I_t := \operatorname{I}[X_0, X_t | X_1 ... X_{t-1}]$, and repeatedly applying the chain rule of conditional mutual information, we obtain
%\begin{equation}
%	P = \sum_{t_- = -T}^{-1} \sum_{t_+ = 0}^\infty \operatorname{I}[X_{t_+},X_{t_-}|X_{t_-+1}\dots X_{t_+-1}] = \sum_{t_- = -T}^{-1} \sum_{t_+ = 0}^\infty I_{t_+-t_-}  = \sum_{t=1}^\infty \min(t,T) I_t = E_0 - \sum_{t > T} (t-T) I_t
%\end{equation}
%where $E_0$ is again the excess entropy.
%Again, this asymptotes to the $E_0$, but never achieves predictiveness $E_0$ at finite $T$.
%The code has rate
%\begin{equation}
%	R =	\operatorname{H}[X_{-T...-1}]
%\end{equation}
%which is lower bounded by $T$ times the entropy rate: $R \geq T \cdot \operatorname{H}[X_0|\past]$, and thus is unbounded, assuming that the entropy rate is nonzero.
%As $T$ grows, this code thus shows diminishing returns in predictiveness $P$ at linear growth in rate $R$.

%Approximating $t$ as a continuous variable, we can write
%\begin{equation}
%	P =	E_0 - \int_{t > T} (t-T) I_t
%\end{equation}
%\begin{equation}
%	\frac{\partial P}{\partial T} =	\int_{t > T} I_t
%\end{equation}
%\begin{equation}
%	\frac{\partial R}{\partial T} \approx \operatorname{H}[X_0|\past]
%\end{equation}
%and thus
%\begin{equation}
%	\frac{\partial P}{\partial R} \approx	\frac{1}{\operatorname{H}[X_0|\past]} \int_{t > T} I_t
%\end{equation}
%Matching this with~(\ref{eq:derivatives}), we get
%\begin{equation}
%	\exp\left(-T^{1/\beta} \frac{1}{\alpha^{1/\beta}} (\operatorname{H}[X_0|\past])^{1/\beta}\right) \propto \int_{t > T} I_t
%\end{equation}
%and thus
%\begin{equation}
%I_t \propto T^{1/\beta-1}	\exp\left(-T^{1/\beta} \frac{1}{\alpha^{1/\beta}} (\operatorname{H}[X_0|\past])^{1/\beta}\right)
%\end{equation} 


\subsection{Word-level language modeling}\label{sec:words}


We applied NPRD to the problem of predicting English on the level of part-of-speech tags in Section~\ref{sec:pos}.
%We verified that NPRD matches OCF in the low-rate regime where OCF is still applicable, and we fitted 
We found that the resulting curves were described well by Equation~\ref{eq:derivatives}.
%\begin{equation}\label{eq:derivatives}
%	\partial P/\partial R = \exp\left(-\frac{1}{\alpha^{1/\beta}} R^{1/\beta}\right). %&\ \ \ \partial R/\partial P = \exp\left(\frac{1}{\alpha^{1/\beta}} R^{1/\beta}\right)
%\end{equation}
We now consider the more realistic problem of prediction at the level of words, using data from multiple languages.
This problem is much closer to the task faced by a human in the process of comprehending text, having to encode prior observations so as to minimize prediction loss on the upcoming words.
We will examine whether Equation~\ref{eq:derivatives} describes the resulting tradeoff in this more realistic setting, and whether it holds across languages.

For the setup, we followed standard setup for recurrent neural language modeling.
The hyperparameters are shown in Table~\ref{tab:nprd-hyperparameters}.
Following standard practice in neural language modeling, we restrict the observation space to the most frequent $10^4$ words; other words are replaced by their part-of-speech tag.
We do this for simplicity and to stay close to standard practice in natural language processing; NPRD could deal with unbounded state spaces through a range of more sophisticated techniques such as subword modeling and character-level prediction~\citep{kim2016character,luong2016achieving}.


We used data from five diverse languages.
For English, we turn to the Wall Street Journal portion of the Penn Treebank \citep{marcus-building-1993}, a standard benchmark for language modeling, containing about 1.2 million tokens.
For Arabic, we pooled all relevant portions of the Universal Dependencies treebanks~\citep{nivre-universal-2016,maamouri2004penn,hajic2004prague}. We obtained 1 million tokens.
We applied the same method to construct a Russian corpus~\citep{syntagrus}, obtaining 1.2 million tokens.
For Chinese, we use the Chinese Dependency Treebank~\citep{che2012chinese}, containing 0.9 million tokens.
For Japanese, we use the first 2 million words from a large processed corpus of Japanese business text~\citep{graff1995japanese}.
For all these languages, we used the predefined splits into training, validation, and test sets.

For each language, we sampled about 120 values of $\log \frac{1}{\lambda}$ uniformly from $[-6, 0]$ and applied NPRD to these.
The resulting curves are shown in Figures~\ref{fig:wordlevel-fit-1}-\ref{fig:wordlevel-fit-2}, together with fitted curves resulting from Equation~\ref{eq:derivatives}.
As can be seen, the curves are qualitatively very similar across languages to what we observed in Figure~\ref{fig:eng-pos-fitted}:
In all languages, rate initially scales linearly with predictiveness, but diverges as the predictiveness approaches its supremum $E_0$.
As a function of $\log \frac{1}{\lambda}$, rate grows at slightly superlinear speed, confirming our ansatz (\ref{eq:r-alpha-beta}).

These results confirm our results from Section~\ref{sec:pos}.
At the time scale of individual sentences, predictive rate-distortion of natural language appears to quantitatively follow Equation~\ref{eq:derivatives}.
NPRD reports rates up to $\approx 60$ nats, more than ten times the largest values of predictiveness.
On the other hand, the growth of rate with predictiveness is relatively gentle in the low-rate regime.
%At the level of individual sentences, prediction
We conclude that predicting words in natural language can be approximated with small memory capacity, but more accurate prediction requires very fine-grained memory representations.

\subsection{General Discussion}

Our analysis of PRD curves for natural language suggests that human language is characterized by very high and perhaps infinite statistical complexity, beyond its excess entropy. In a similar vein, \citet{dkebowski2018natural} has argued that the excess entropy of connected texts in natural language is infinite (in contrast, our result is for isolated sentences). If the statistical complexity of natural language is indeed infinite, then statistical complexity is not sufficiently fine-grained as a complexity metric for characterizing natural language. 

We suggest that the PRD curve may form a more natural complexity metric for highly complex processes such as language. Among those processes with infinite statistical complexity, some will have a gentle PRD curve---meaning that they can be well-approximated at low rates---while others will have a steep curve, meaning they cannot be well-approximated at low rates. We conjecture that although natural language may have infinite statistical complexity, it has a gentler PRD curve than other processes with this property, meaning that achieving a reasonable approximation of the predictive distribution does not require inordinate memory resources.

%
%\begin{table}
%	\begin{tabular}{lllllll}
%		& $\alpha$ & $\beta$ & $E_0$ \\
%		English         & \\
%		Russian & \\
%		Arabic & \\
%		Japanese & 
%		CHinese & \\
%	\end{tabular}
%
%\end{table}

%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/en-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/en-words-16-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/en-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/en-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/en-words-logbeta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/en-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of English.}\label{fig:eng-logbeta}
%\end{figure*}
%
%
%
%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-16-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-logbeta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of Russian.}\label{fig:rug-logbeta}
%\end{figure*}
%
%
%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-16-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-logbeta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of Japanese.}\label{fig:jap-logbeta}
%\end{figure*}
%
%
%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-16-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-logbeta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of Chinese.}\label{fig:jap-logbeta}
%\end{figure*}
%
%
%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-16-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ar-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ar-words-logbeta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/ar-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of Arabic.}\label{fig:jap-logbeta}
%\end{figure*}
%
%
%
%
%
%
%\begin{figure*}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-surp-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-16-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-ee-mem.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/ru-words-beta-mem.pdf}
%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-nlogbeta-mem-fitted.pdf}
%%\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-logbeta-ee.pdf}
%	\caption{Word-level modeling of Japanese.}\label{fig:jap-logbeta-fit}
%\end{figure*}
%



\begin{figure*}
	\begin{center}
		English {\tiny{($\alpha=2.0$, $\beta=1.9$, $E_0=3.5$)}}


\includegraphics[width=0.3\textwidth]{code/figures/en-words-info-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/en-words-nlogbeta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/en-words-nlogbeta-ee-fitted.pdf}

		Russian {\tiny ($\alpha=0.5$, $\beta=2.3$, $E_0=1.7$)}


	\includegraphics[width=0.3\textwidth]{code/figures/ru-words-info-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/ru-words-nlogbeta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/ru-words-nlogbeta-ee-fitted.pdf}

		Arabic {\tiny ($\alpha=1.5$, $\beta=2.0$, $E_0=2.9$)}

		\includegraphics[width=0.3\textwidth]{code/figures/ar-words-info-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/ar-words-nlogbeta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/ar-words-nlogbeta-ee-fitted.pdf}
	\end{center}
	\caption{Word-level results.}\label{fig:wordlevel-fit-1}
\end{figure*}



\begin{figure*}
	\begin{center}
		Japanese {\tiny ($\alpha=2.0$, $\beta=2.0$, $E_0=4.5$)}


\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-info-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-nlogbeta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/LDC95T8-words-nlogbeta-ee-fitted.pdf}

		Chinese {\tiny ($\alpha=1.0$, $\beta=2.3$, $E_0=3.1$)}


		\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-info-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-nlogbeta-mem-fitted.pdf}
\includegraphics[width=0.3\textwidth]{code/figures/LDC2012T05-words-nlogbeta-ee-fitted.pdf}
	\end{center}
	\caption{Word-level results (cont.).}\label{fig:wordlevel-fit-2}
\end{figure*}






\section{Conclusion}
We introduce Neural Predictive Rate-Distortion (NPRD), a method for estimating predictive rate-distortion when only sample trajectories are given.
Unlike OCF, the most general prior method, NPRD scales to long sequences and large state spaces.
On analytically tractable processes, we show that it closely fits the analytical rate-distortion curve and recovers the causal states of the process.
On part-of-speech-level modeling of natural language, it agrees with OCF in the setting of low rate and short sequences; outside these settings, OCF fails due to combinatorial explosion and overfitting, while NPRD continues to provide estimates.
Finally, we use NPRD to provide the first estimates of predictive rate-distortion for modeling natural language in five different languages, finding qualitatively very similar curves in all languages.
%Results suggest that prediction can be approximated with sim, but more accurate prediction requires very fine-grained memory representations.

All code for reproducing the results in this work is available at \url{https://github.com/m-hahn/predictive-rate-distortion}.


%\bibliographystyle{apalike}
%\bibliography{extracted}
%\bibliography{literature}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Methodology, Michael Hahn; Writing  original draft, Michael Hahn and Richard Futrell.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\funding{This research received no external funding.} %'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{We thank Dan Jurafsky for helpful discussion, and the anonymous reviewers for helpful comments.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest.} %Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript, or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\abbreviations{The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
NPRD & Neural Predictive Rate-Distortion\\
OCF & Optimal Causal Filtering\\
PRD & Predictive Rate-Distortion \\
LSTM & Long Short Term Memory
\end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\appendixtitles{no} %Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixsections{multiple} %Leave argument "multiple" if there are multiple sections. Then a counter is printed ("Appendix A"). If there is only one appendix section then change the argument to "one" and no counter is printed ("Appendix").
\appendix
\section{Hyperparameters}\label{sec:app-hyperparams}

All hyperparameter choices are shown in Table~\ref{tab:nprd-hyperparameters}.
We defined separate hyperparameter ranges for the three Sections~\ref{sec:tractable}, \ref{sec:pos}, and \ref{sec:words}.
Guided by the fact that the analytically known processes in Section~\ref{sec:tractable} are arguably less complex than natural language, we allowed larger and more powerful models for modeling of natural language (Sections \ref{sec:pos}, \ref{sec:words}), in particular word-level modeling (Section~\ref{sec:words}).

In Table~\ref{tab:nprd-hyperparameters}, hyperparameters are organized into four groups.
The first group of parameters are the dimensions of the input embedings and the recurrent LSTM states.
The second group of parameters are regularization parameters.
We apply dropout~\citep{srivastava-dropout:-2014} to the input and output layers.
The third group of parameters are related to the optimization procedure~\citep{kingma-adam:-2014}.
Neural Autoregressive Flows, used to approximate the marginal $q$, also have a set of hyperparameters \citep{huang-neural-2018}: The length of the flow (1, 2, ,...), the type (DSF / Deep Sigmoid Flow or  DDSF / Deep Dense Sigmoid Flow), the dimension of the flow (an integer), and the number of layers in the flow (1,2,...).
Larger dimensions, more layers, and longer flows lead to more expressive models; however, they are computationally more expensive.

Training used Early Stopping using the development set, as described in Section~\ref{sec:parameter-estimation}.
Models were trained on a TITAN Xp graphics card.
\mhahn{TODO number of epochs, training time}

On English word-level modeling, training took a median number of 9 epochs (max 467 epochs) and 5 minutes (max 126 minutes).



\begin{table}
\begin{tabular}{l||lll}
	& Section~\ref{sec:tractable} &   Section~\ref{sec:pos} & Section~\ref{sec:words}\\ \hline\hline
	Embedding Dimension & 50, 100 &    50, 100, 200, 300   &  150      \\
	LSTM Dimension & 32           &   64, 128, 256, 512    &   256, 512     \\ \hline
%	LSTM Layers & 1 \\ \hline
	Dropout rate & 0.0, 0.1, 0.2  &   0.0, 0.1, 0.2    &   0.1,   0.4     \\
	Input Dropout & 0  &    0.0, 0.1, 0.2   &        0.2             \\ \hline
	Adam Learning Rate & $\{1,5\} \cdot 10^{-4}$, $\{1,2,4\} \cdot 10^{-3}$  &   $\{1,5\} \cdot 10^{-4}$, $\{1,2,4\} \cdot 10^{-3}$ &       0.00005, 0.0001, 0.0005, 0.001     \\
	Batch Size & 16, 32, 64  &    16, 32 64   & 16, 32, 64  \\ \hline
	Flow Length & 1, 2  &   1,2,3,4,5    &          1,2,3,4,5              \\
	Flow Type & DSF, DDSF  &    DSF, DDSF   &       DSF, DDSF             \\
	Flow Dimension & 32, 64, 128, 512  &   512    &    512        \\
	Flow Layers & 2  &    2   &                 2         \\
\end{tabular}
	\caption{NPRD Hyperparameters. See Appendix~\ref{sec:app-hyperparams} for description of the parameters.}\label{tab:nprd-hyperparameters}
\end{table}



\section{Alternative Modeling Choices}\label{sec:app-choices}

In this section, we investigate the trade-offs involved in alternative modeling choices. It may be possible to use simpler function-approximators for $\phi$, $\psi$, and $q$, or smaller context windows sizes $M$, without harming accuracy too much.

First, we investigated the performance of a simple fixed approximation to the marginal $q$.
We considered a diagonal unit-variance Gaussian, as is common in the literature on Variational Autoencoders~\citep{kingma-auto-encoding-2014}.
We show results in Figure~\ref{fig:even-noFlow}.
Even with this fixed approximation to $q$, NPRD continues to provides estimates not far away from the analytical curve.
However, comparison with the results obtained from full NPRD (Figure~\ref{fig:even}) shows that a flexible parameterized approximation still provides considerably better fit.

Second, we investigated whether the use of recurrent neural networks is necessary.
As recurrent models such as LSTMs process input sequentially, they cannot be fully parallelized, posing the question whether they can be replaced by models that can be parallelized.
Specifically, we considered \key{Quasi-Recurrent Neural Networks} (QRNNs), which combine convolutions with a weak form of recurrence, and which have shown strong results on language modeling benchmarks~\citep{bradbury2017quasi}.
We replaced the LSTM encoder and decoder with QRNNs and fit NPRD to the Even Process and the Random Insertion Process.
We found that when using QRNNs, NPRD consistently fitted codes with zero rate for the Even Process and the Random Insertion Process, indicating that the QRNN was failing to extract useful information from the past of these processes.
We also found that the cross-entropies of the estimated marginal distribution $P_\eta(\finitepast)$ were considerably worse than when using LSTMs or simple RNNs.
We conjecture that, due to the convolutional nature of QRNNs, they cannot model such processes effectively in principle:
QRNNs extract representations by pooling embeddings of words or $n$-grams occurring in the past.
When modeling, e.g., the Even Process, the occurrence of specific $n$-grams is not informative about whether the length of the last block is even or odd; this requires some information about the positions in which these $n$-grams occur, which is not available to the QRNN.
\rljf{We'll want a bit more detail on this: someone might think we just made some mistake in applying the QRNNs.} \mhahn{I've tried to make it more explicit, is this okay?}

Third, we varied $M$, comparing $M=5, 10$ to the results obtained with $M=15$. 
Results are shown in Figure~\ref{fig:even-varyM}.
At $M=5$, NPRD provides estimates similar to OCF (Figure~\ref{fig:even}).
At $M=10$, the estimates are close to the analytical curve; nonetheless, $M=15$ yields clearly more accurate results.


\begin{figure*}
\centering
\includegraphics[width=0.45\textwidth]{code/figures/even-info-noFlow.pdf}
\includegraphics[width=0.45\textwidth]{code/figures/rip-info-noFlow.pdf}

%\includegraphics[width=0.45\textwidth]{code/figures/even-beta-mem.pdf}
	\caption{Rate-Distortion for the Even Process (left) and the Random Insertion Process (right), estimated using a simple diagonal unit Gaussian approximation for $q$. Gray lines: Analytical curves. Red dots: Multiple runs of NPRD ($> 200$ samples). Red line: Tradeoff curve computed from NPRD runs. Compare Figure~\ref{fig:even} for results from full NPRD. }\label{fig:even-noFlow}
\end{figure*}



\begin{figure*}
\centering
\includegraphics[width=0.45\textwidth]{code/figures/even-info-varyM.pdf}
\includegraphics[width=0.45\textwidth]{code/figures/rip-info-varyM.pdf}

%\includegraphics[width=0.45\textwidth]{code/figures/even-beta-mem.pdf}
	\caption{Rate-Distortion for the Even Process (left) and the Random Insertion Process (right), varying $M=5$ (blue), $10$ (red), $15$ (green). Gray lines: Analytical curves. Red dots: Multiple runs of NPRD. Red line: Tradeoff curve computed from NPRD runs. Compare Figure~\ref{fig:even} for results from full NPRD.}\label{fig:even-varyM}
\end{figure*}


\section{Sample Runs on English Text}\label{sec:english-samples}





\begin{figure*}
\centering
within minutes after the stock market closed friday , i called sen. bill bradley of new
\includegraphics[width=0.95\textwidth]{code/nprd/figures/sample_0.png}


asset you own would be worth more if the government suddenly announced that if you sold
\includegraphics[width=0.95\textwidth]{code/nprd/figures/sample_20.png}


, OOV the office of richard darman , director of the office of management and budget
\includegraphics[width=0.95\textwidth]{code/nprd/figures/sample_13.png}


he thinks , when the white house is forced to ask for higher taxes to meet
\includegraphics[width=0.95\textwidth]{code/nprd/figures/sample_31.png}

	\caption{}\label{fig:samples-ptb}
\end{figure*}


%\unskip
%\subsection{}
%The appendix is an optional section that can contain details and data supplemental to the main text. For example, explanations of experimental details that would disrupt the flow of the main text, but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data is shown in the main text can be added here if brief, or as Supplementary data. Mathematical proofs of results not central to the paper can be added as an appendix.

%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: internal bibliography
%=====================================
\reftitle{References}

% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%=====================================
% References, variant B: external bibliography
%=====================================
\externalbibliography{yes}
\bibliography{literature}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\sampleavailability{Code is available.}

%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors response\\
%Reviewer 2 comments and authors response\\
%Reviewer 3 comments and authors response
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}



